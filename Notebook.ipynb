{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41571ce8",
   "metadata": {},
   "source": [
    "<img src=\"Data/sentiment_picture.png\" width=\"700\"/>\n",
    "\n",
    " # Social Media Sentiment Analysis\n",
    " # Group 3 \n",
    "\n",
    "Authors\n",
    "  - Ibrahim Salim\n",
    "  - Abond Mwangi\n",
    "  - Abigail Muthenya\n",
    "  - Nelson Kamau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e060fc1",
   "metadata": {},
   "source": [
    "# **1.0 Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83653aac",
   "metadata": {},
   "source": [
    "In today’s digital age, social media has become a critical channel for customers to share their opinions, experiences, and frustrations with brands and products. Platforms like Twitter generate vast amounts of unstructured text data daily, reflecting public sentiment in real time. For businesses, monitoring and understanding this sentiment is essential to protect brand reputation, respond to customer concerns, and inform strategic decisions. However, the sheer volume and velocity of social media data make it challenging to track and analyze sentiment manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab4906e",
   "metadata": {},
   "source": [
    "## 1.1 Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaef490",
   "metadata": {},
   "source": [
    "The goal of this project is to design and implement an automated system that will determine the sentiment in tweets which are directed towards specific brands such as Borderlands, Facebook, Amazon, and Microsoft. The system will be examining a large body of tweets, determining the overall sentiment in every tweet, and labeling  into sentiment categories such as Positive, Negative, Neutral, or Irrelevant. By leveraging Natural Language Processing and machine learning, the system will help organizations gain timely insights into how the public perceives their products and services.\n",
    "\n",
    "The results from this system can be employed to provide actionable insights to key stakeholders like brand managers, marketing departments, and community managers so that they can:\n",
    "\n",
    "- Monitor public opinion about their brand or product on an ongoing basis.\n",
    "\n",
    "- Recognize future potential issues or spikes in negative sentiment.\n",
    "\n",
    "- Measure the impact of marketing initiatives or product releases.\n",
    "\n",
    "- Compare sentiment trends relative to competition.\n",
    "\n",
    "The project will make use of a labeled set of tweets for training and validation. The system's performance will be measured by how well the system will be able to classify sentiment in order to instill confidence in businesses with regard to insights produced. Ultimately, the project demonstrates how data-driven sentiment analysis can facilitate proactive decision-making in brand management and customer engagement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb567871",
   "metadata": {},
   "source": [
    "## 1.2 The Business Stakeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36850f",
   "metadata": {},
   "source": [
    "This project has been initiated at the request of **Interbrand**, a global leading brand consultancy that helps businesses develop, manage, and protect their brand reputation. Interbrand brand managers monitor how the public perceives their clients' brands through all forms of media, including social media sites like Twitter. With the overwhelming amount of real-time customer discussions online, Interbrand's brand managers require an automated solution that can efficiently classify sentiment in tweets that talk about their clients' services or products. \n",
    "\n",
    "The other key stakeholders are **Interbrand**'s marketing analysts and PR teams, who will utilize these findings to make campaign strategy, crisis management strategy, and competitor benchmarking reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a511dd",
   "metadata": {},
   "source": [
    "## 1.3 Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16523bf7",
   "metadata": {},
   "source": [
    "In a time where public opinion can shift in a matter of seconds, brands need to be constantly on guard for what people are saying about them online. Social networks like Twitter generate massive streams of live comment, complaint, compliment, and conversation that can influence a brand's reputation, customer loyalty, and market position.\n",
    "\n",
    "But it is not practical and scalable to track and analyze manually thousands of tweets each day. This is very manual-intensive for brand managers and consultancies like **Interbrand** who are responsible for overseeing many brands in various industries and markets. Without a reliable way of monitoring the sentiment from these discussions, businesses are at risk of missing early warning signs of negative trends, missing important customer insights, or failing to properly gauge the full effects of their campaigns.\n",
    "\n",
    "This project answers that challenge by developing an automated sentiment analysis system that is able to classify tweets mentioning a brand into precise sentiment categories: **Positive, Negative, Neutral, or Irrelevant**. Through transforming raw, unstructured text data into actionable information, this system will enable brand managers to:\n",
    "\n",
    "- Track sentiment trends over time.\n",
    "\n",
    "- Identify sudden spikes in negative or positive remarks.\n",
    "\n",
    "- Respond quickly to breaking issues or crises.\n",
    "\n",
    "- Use data-driven insights to make decisions regarding communication strategies, product development, and customer engagement.\n",
    "\n",
    "Ultimately, the outcome is to allow brand managers to make timely, data-informed decisions that drive better decision-making and help them defend and build their brands in a changing digital world. By the end of this project we hope to uncover questions such as:\n",
    "\n",
    "- What is the overall sentiment towards the brand(s) on Twitter over a given period?\n",
    "\n",
    "- Are there specific topics, events, or product launches that cause spikes in positive or negative sentiment?\n",
    "\n",
    "- Can sudden surges in negative sentiment be detected early enough to enable timely response by customer support or PR teams?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a50190",
   "metadata": {},
   "source": [
    "## 1.4 Project Objectives "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf7339",
   "metadata": {},
   "source": [
    "The primary objective of this project is to develop an **automated sentiment analysis system** capable of classifying tweets that mention specific brands into distinct sentiment categories: **Positive, Negative, Neutral, or Irrelevant**. This will enable **brand managers and consultancies** like Interbrand to transform large volumes of unstructured social media data into actionable insights.\n",
    "\n",
    "To achieve this, the project will pursue the following specific objectives:\n",
    "\n",
    "- Build and train a reliable sentiment classification model using a labeled dataset of tweets that mention various brands, including Borderlands, Facebook, Amazon, and others.\n",
    "\n",
    "- Validate and evaluate the model’s performance to ensure it achieves high accuracy and can generalize well to new, unseen tweets.\n",
    "\n",
    "- Provide clear, interpretable outputs that brand managers can use to monitor brand health, inform marketing and PR strategies, and respond quickly to emerging issues.\n",
    "\n",
    "- Develop recommendations for how this system could be integrated into the stakeholders’ existing workflows for continuous, real-time sentiment tracking.\n",
    "\n",
    "By fulfilling these objectives, the project aims to demonstrate how automated sentiment analysis can support proactive, data-driven brand management in an increasingly fast-paced digital environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341de849",
   "metadata": {},
   "source": [
    "# **2.0 Data Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540a67f",
   "metadata": {},
   "source": [
    "The dataset used in this project was sourced from **Kaggle’s Twitter Entity Sentiment Analysis collection**. It includes labeled tweets that refer to different brands, firms, or products, which were categorized into one of four sentiment types: **Positive, Negative, Neutral, or Irrelevant**.\n",
    "\n",
    "The dataset includes two training and validation CSV files combined, which have over 75,000 tweets. The shape and composition of this dataset are important for determining how to build a robust sentiment classification model. This section will describe salient characteristics of the data, detect any potential issues (such as class imbalance or redundant entries), and lay the foundation for the data preprocessing and modeling sections that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fbda1",
   "metadata": {},
   "source": [
    "## 2.1 Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ccc85",
   "metadata": {},
   "source": [
    "The data includes two CSV files, one for **training (twitter_training.csv)** and one for **validation (twitter_validation.csv)** ,with a total of over 75,000 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d0713",
   "metadata": {},
   "source": [
    "**twitter_training.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf72c73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Positive</th>\n",
       "      <th>im getting on borderlands and i will murder you all ,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2401  Borderlands  Positive  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "...     ...          ...       ...   \n",
       "74676  9200       Nvidia  Positive   \n",
       "74677  9200       Nvidia  Positive   \n",
       "74678  9200       Nvidia  Positive   \n",
       "74679  9200       Nvidia  Positive   \n",
       "74680  9200       Nvidia  Positive   \n",
       "\n",
       "      im getting on borderlands and i will murder you all ,  \n",
       "0      I am coming to the borders and I will kill you...     \n",
       "1      im getting on borderlands and i will kill you ...     \n",
       "2      im coming on borderlands and i will murder you...     \n",
       "3      im getting on borderlands 2 and i will murder ...     \n",
       "4      im getting into borderlands and i can murder y...     \n",
       "...                                                  ...     \n",
       "74676  Just realized that the Windows partition of my...     \n",
       "74677  Just realized that my Mac window partition is ...     \n",
       "74678  Just realized the windows partition of my Mac ...     \n",
       "74679  Just realized between the windows partition of...     \n",
       "74680  Just like the windows partition of my Mac is l...     \n",
       "\n",
       "[74681 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training CSV file\n",
    "file_path = 'data/twitter_training.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe72ca",
   "metadata": {},
   "source": [
    "**twitter_validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140d116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3364</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>Irrelevant</th>\n",
       "      <th>I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>352</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4371</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4433</td>\n",
       "      <td>Google</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6273</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hi @EAHelp I’ve had Madeleine McCann in my cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>4891</td>\n",
       "      <td>GrandTheftAuto(GTA)</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4359</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2652</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>8069</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>6960</td>\n",
       "      <td>johnson&amp;johnson</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     3364             Facebook  Irrelevant  \\\n",
       "0     352               Amazon     Neutral   \n",
       "1    8312            Microsoft    Negative   \n",
       "2    4371                CS-GO    Negative   \n",
       "3    4433               Google     Neutral   \n",
       "4    6273                 FIFA    Negative   \n",
       "..    ...                  ...         ...   \n",
       "994  4891  GrandTheftAuto(GTA)  Irrelevant   \n",
       "995  4359                CS-GO  Irrelevant   \n",
       "996  2652          Borderlands    Positive   \n",
       "997  8069            Microsoft    Positive   \n",
       "998  6960      johnson&johnson     Neutral   \n",
       "\n",
       "    I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣  \n",
       "0    BBC News - Amazon boss Jeff Bezos rejects clai...                                                                                                                                                                                                  \n",
       "1    @Microsoft Why do I pay for WORD when it funct...                                                                                                                                                                                                  \n",
       "2    CSGO matchmaking is so full of closet hacking,...                                                                                                                                                                                                  \n",
       "3    Now the President is slapping Americans in the...                                                                                                                                                                                                  \n",
       "4    Hi @EAHelp I’ve had Madeleine McCann in my cel...                                                                                                                                                                                                  \n",
       "..                                                 ...                                                                                                                                                                                                  \n",
       "994  ⭐️ Toronto is the arts and culture capital of ...                                                                                                                                                                                                  \n",
       "995  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...                                                                                                                                                                                                  \n",
       "996  Today sucked so it’s time to drink wine n play...                                                                                                                                                                                                  \n",
       "997  Bought a fraction of Microsoft today. Small wins.                                                                                                                                                                                                  \n",
       "998  Johnson & Johnson to stop selling talc baby po...                                                                                                                                                                                                  \n",
       "\n",
       "[999 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the validation CSV file \n",
    "val_df = pd.read_csv('Data/twitter_validation.csv')\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240830f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2401', 'Borderlands', 'Positive',\n",
       "       'im getting on borderlands and i will murder you all ,'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the columns of the training DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcbe9dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['3364', 'Facebook', 'Irrelevant',\n",
       "       'I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the columns of the validation DataFrame\n",
    "val_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff534fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Entity', 'Sentiment', 'Text'], dtype='object')\n",
      "Index(['ID', 'Entity', 'Sentiment', 'Text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Change the column names to match the expected format\n",
    "\n",
    "df.columns = ['ID', 'Entity', 'Sentiment', 'Text']\n",
    "val_df.columns = ['ID', 'Entity', 'Sentiment', 'Text']\n",
    "\n",
    "# Display the updated columns\n",
    "print(df.columns)\n",
    "print(val_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f54e4a",
   "metadata": {},
   "source": [
    "## 2.2 Data Dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a78ffc",
   "metadata": {},
   "source": [
    "- **ID**: A unique identifier for the tweet.\n",
    "\n",
    "- **Entity**: The brand, company, or product mentioned (e.g., Borderlands, Facebook, Amazon).\n",
    "\n",
    "- **Sentiment**: The sentiment label assigned to the tweet.\n",
    "\n",
    "- **Tweet Text**: The actual tweet content, representing the raw, unstructured text data to be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a8fc6",
   "metadata": {},
   "source": [
    "## 2.3 Data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d58eec8",
   "metadata": {},
   "source": [
    "Here we shall be exploring the datasets to have better understanding about what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d2fa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74681 entries, 0 to 74680\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ID         74681 non-null  int64 \n",
      " 1   Entity     74681 non-null  object\n",
      " 2   Sentiment  74681 non-null  object\n",
      " 3   Text       73995 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0fdab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 999 entries, 0 to 998\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ID         999 non-null    int64 \n",
      " 1   Entity     999 non-null    object\n",
      " 2   Sentiment  999 non-null    object\n",
      " 3   Text       999 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 31.3+ KB\n"
     ]
    }
   ],
   "source": [
    "val_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4f3b9",
   "metadata": {},
   "source": [
    "**Check for missing entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b09c4c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0.000000\n",
       "Entity       0.000000\n",
       "Sentiment    0.000000\n",
       "Text         0.918574\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the percentage of missing values per column for the training dataset\n",
    "100*(df.isnull().sum()/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2e8cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0.0\n",
       "Entity       0.0\n",
       "Sentiment    0.0\n",
       "Text         0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the percentage of missing values per column for the validation dataset\n",
    "100*(val_df.isnull().sum()/len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe1b71",
   "metadata": {},
   "source": [
    "**Check for duplicated entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ce7ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for duplicated entries in the training dataset\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4acf9637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicated entries in the validation dataset\n",
    "val_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b0a45",
   "metadata": {},
   "source": [
    "# **3.0 Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8632e5",
   "metadata": {},
   "source": [
    "Before building a stable sentiment classification model, it is important to pre-process the raw Twitter data for analysis. This stage involves cleaning, transforming, and structuring the dataset to ensure it is suitable for machine learning. The primary tasks are handling missing or duplicate records, normalizing text data and class imbalance handling. Effective data preparation lays the groundwork for building a robust and accurate sentiment analysis system that aligns with the project’s business objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a209c",
   "metadata": {},
   "source": [
    "## 3.1 Handling missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2df33bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212d355",
   "metadata": {},
   "source": [
    " ## 3.2 Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601f57d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # remove mentions\n",
    "        text = re.sub(r'#[A-Za-z0-9_]+', '', text)  # remove hashtags\n",
    "        text = re.sub(r'http\\S+', '', text)         # remove URLs\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)        # remove non-letters\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [t for t in tokens if t not in self.stop_words]\n",
    "        tokens = [self.lemmatizer.lemmatize(t, pos='v') for t in tokens]\n",
    "        return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a1fa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  I am coming to the borders and I will kill you...   \n",
      "1  im getting on borderlands and i will kill you ...   \n",
      "2  im coming on borderlands and i will murder you...   \n",
      "3  im getting on borderlands 2 and i will murder ...   \n",
      "4  im getting into borderlands and i can murder y...   \n",
      "\n",
      "                   clean_text  \n",
      "0            come border kill  \n",
      "1     im get borderlands kill  \n",
      "2  im come borderlands murder  \n",
      "3   im get borderlands murder  \n",
      "4   im get borderlands murder  \n"
     ]
    }
   ],
   "source": [
    "cleaner = TextCleaner()\n",
    "\n",
    "df['clean_text'] = df['Text'].apply(cleaner.clean)\n",
    "print(df[['Text', 'clean_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4dd97",
   "metadata": {},
   "source": [
    "## 3.3 Handling the duplicated entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33206dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12221"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count duplicate cleaned tweets\n",
    "df.duplicated(subset=['clean_text']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6926ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on cleaned text\n",
    "df.drop_duplicates(subset=['clean_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99f04820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(\"twitter_training_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6139147",
   "metadata": {},
   "source": [
    "## 3.4 Check for class imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10bd7029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "Negative      30.580827\n",
      "Positive      26.519248\n",
      "Neutral       25.182115\n",
      "Irrelevant    17.717810\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['Sentiment'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8826e",
   "metadata": {},
   "source": [
    "# **4.0 Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75176a",
   "metadata": {},
   "source": [
    "With the data cleaned, prepared, and explored, the next step is to develop and evaluate machine learning models that can accurately classify the sentiment of tweets. This stage involves transforming the text data into numerical features using **natural language processing (NLP)** techniques, selecting suitable algorithms, training the models on labeled examples, and validating their performance.\n",
    "\n",
    "The goal is to identify a model that can generalize well to new, unseen tweets and reliably predict whether a tweet’s sentiment is Positive, Negative, Neutral, or Irrelevant. This section will cover the text vectorization process, model selection, training, and performance evaluation to ensure the final sentiment classifier meets the business requirements for accuracy and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe30802",
   "metadata": {},
   "source": [
    "## 4.1 Vectorize text (TF-IDF) and prepare the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d0cee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "#  Prepare target variable\n",
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0504a795",
   "metadata": {},
   "source": [
    " ## 4.2 Building the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbd89d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.65      0.53      0.58      2150\n",
      "    Negative       0.72      0.78      0.75      3876\n",
      "     Neutral       0.65      0.62      0.64      3037\n",
      "    Positive       0.68      0.72      0.70      3292\n",
      "\n",
      "    accuracy                           0.68     12355\n",
      "   macro avg       0.67      0.66      0.67     12355\n",
      "weighted avg       0.68      0.68      0.68     12355\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1140  373  278  359]\n",
      " [ 186 3022  361  307]\n",
      " [ 229  454 1897  457]\n",
      " [ 197  337  391 2367]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5e616",
   "metadata": {},
   "source": [
    "**Prediction example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5df1aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Assuming the TextCleaner class is already defined and imported\n",
    "\n",
    "# Create cleaner object\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "# Example text\n",
    "example = [\"I absolutely love the new Borderlands game!\"]\n",
    "\n",
    "# Clean the text\n",
    "example_clean = [cleaner.clean(example[0])]\n",
    "\n",
    "# Vectorize and predict\n",
    "example_vec = vectorizer.transform(example_clean)\n",
    "prediction = model.predict(example_vec)\n",
    "\n",
    "print(\"\\nPredicted sentiment:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d31ff33",
   "metadata": {},
   "source": [
    "The baseline model demonstrates a reasonable starting point, achieving an overall accuracy of approximately 68% and performing strongest on Negative and Positive sentiment classes. However, the relatively lower recall for the Irrelevant and Neutral categories indicates that the model struggles to distinguish these sentiments reliably. This suggests that further improvements in text preprocessing, feature extraction, and model tuning could help enhance overall performance and reduce misclassifications. In the next steps, additional techniques such as advanced vectorization, hyperparameter optimization, or handling potential class imbalances will be explored to build a more robust and generalizable sentiment classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af80f95",
   "metadata": {},
   "source": [
    "#  4.3 Random Forest (with bi-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaed2703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.88      0.83      0.85      2150\n",
      "    Negative       0.89      0.93      0.91      3876\n",
      "     Neutral       0.92      0.88      0.90      3037\n",
      "    Positive       0.87      0.89      0.88      3292\n",
      "\n",
      "    accuracy                           0.89     12355\n",
      "   macro avg       0.89      0.88      0.89     12355\n",
      "weighted avg       0.89      0.89      0.89     12355\n",
      "\n",
      "[[1778  126   87  159]\n",
      " [  81 3589   74  132]\n",
      " [  71  147 2677  142]\n",
      " [  86  174   87 2945]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=2000, ngram_range=(1, 2))),\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3aacccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the pipeline\n",
    "\n",
    "with open(\"rf_pipeline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf_pipeline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc36e1",
   "metadata": {},
   "source": [
    "The Random Forest model implemented within a streamlined pipeline, which combines TF-IDF vectorization with bi-grams, delivers a notable performance boost over the baseline. The overall accuracy has increased to 89%, with consistently strong precision, recall, and F1-scores across all sentiment classes. The model now consistently fares better for the Irrelevant class, with 0.83 recall compared to 0.53 in the original baseline. Using a pipeline not only makes the process more efficient but also keeps preprocessing and modeling under the same roof which stabilizes and makes the sentiment classification task reproducible. All these findings determine the impact of more informative text features, class weights balancing, and ensemble learning on detecting subtle patterns in social media text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ff5e2",
   "metadata": {},
   "source": [
    "#  4.4 XGBoost (with bi-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd298396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [07:23:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.70      0.36      0.47      2150\n",
      "    Negative       0.60      0.83      0.70      3876\n",
      "     Neutral       0.68      0.55      0.61      3037\n",
      "    Positive       0.65      0.68      0.66      3292\n",
      "\n",
      "    accuracy                           0.64     12355\n",
      "   macro avg       0.66      0.60      0.61     12355\n",
      "weighted avg       0.65      0.64      0.63     12355\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      " [[ 772  707  222  449]\n",
      " [  89 3211  287  289]\n",
      " [ 113  761 1682  481]\n",
      " [ 129  646  290 2227]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Encode labels first — handled outside the pipeline since LabelEncoder isn't a pipeline step\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline: Vectorizer + Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n",
    "])\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_encoded = pipeline.predict(X_test)\n",
    "\n",
    "# Decode labels for reporting\n",
    "y_pred_decoded = le.inverse_transform(y_pred_encoded)\n",
    "y_test_decoded = le.inverse_transform(y_test)\n",
    "\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test_decoded, y_pred_decoded))\n",
    "print(\"XGBoost Confusion Matrix:\\n\", confusion_matrix(y_test_decoded, y_pred_decoded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14954805",
   "metadata": {},
   "source": [
    "| Metric        | Random Forest         | XGBoost    |\n",
    "| ------------- | --------------------- | ---------- |\n",
    "| Accuracy      | **89%**               | 64%        |\n",
    "| Macro F1      | **0.89**              | 0.61       |\n",
    "| Best Class    | Negative / Irrelevant | Negative   |\n",
    "| Weakest Class | Irrelevant (recall)   | Irrelevant |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c2a784",
   "metadata": {},
   "source": [
    "The XGBoost classifier, implemented through a pipeline with TF-IDF vectorization, achieved an overall accuracy of approximately 64%. While XGBoost is generally a strong gradient boosting algorithm, these results indicate that, in this setup, it did not outperform the Random Forest model. The precision and recall for the Negative class remain strong at 0.60 and 0.83, respectively, but the performance for the Irrelevant class dropped noticeably, with recall falling to 0.36. This suggests that, in this scenario, XGBoost may not be capturing the context and nuance of the cleaned tweet text as effectively as the Random Forest model using bi-grams. These findings highlight the importance of testing multiple models and configurations to determine the most suitable approach for sentiment classification in real-world social media data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb01b5",
   "metadata": {},
   "source": [
    "Once we trained our Random Forest model, it was already performing quite well — achieving approximately 89% accuracy on the validation set. This strong baseline gave us confidence that the model was learning meaningful patterns from the data. However, we also recognized that with some careful hyperparameter tuning, we could potentially improve performance even further to support more accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bec30",
   "metadata": {},
   "source": [
    "## 4.5 Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457fe16",
   "metadata": {},
   "source": [
    "Initially, we attempted to use GridSearchCV, which exhaustively tests all combinations of hyperparameters. While thorough, this approach proved to be too time-consuming given the size of our dataset and the number of parameter options. To balance efficiency with optimization, we instead used RandomizedSearchCV — a faster, more practical method that samples a limited number of random combinations from the parameter space. This allowed us to fine-tune the model more efficiently while still seeking improved predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29226ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=2,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                              TfidfVectorizer(max_features=2000,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           2))),\n",
       "                                             (&#x27;rf&#x27;,\n",
       "                                              RandomForestClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "                                                                     n_estimators=200,\n",
       "                                                                     random_state=42))]),\n",
       "                   n_iter=4, n_jobs=-1,\n",
       "                   param_distributions={&#x27;rf__max_depth&#x27;: [None, 10, 20],\n",
       "                                        &#x27;rf__n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F9C3F584F0&gt;},\n",
       "                   random_state=42, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=2,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                              TfidfVectorizer(max_features=2000,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           2))),\n",
       "                                             (&#x27;rf&#x27;,\n",
       "                                              RandomForestClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "                                                                     n_estimators=200,\n",
       "                                                                     random_state=42))]),\n",
       "                   n_iter=4, n_jobs=-1,\n",
       "                   param_distributions={&#x27;rf__max_depth&#x27;: [None, 10, 20],\n",
       "                                        &#x27;rf__n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F9C3F584F0&gt;},\n",
       "                   random_state=42, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(max_features=2000, ngram_range=(1, 2))),\n",
       "                (&#x27;rf&#x27;,\n",
       "                 RandomForestClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "                                        n_estimators=200, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=2000, ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, n_estimators=200,\n",
       "                       random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=Pipeline(steps=[('tfidf',\n",
       "                                              TfidfVectorizer(max_features=2000,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           2))),\n",
       "                                             ('rf',\n",
       "                                              RandomForestClassifier(class_weight='balanced',\n",
       "                                                                     n_estimators=200,\n",
       "                                                                     random_state=42))]),\n",
       "                   n_iter=4, n_jobs=-1,\n",
       "                   param_distributions={'rf__max_depth': [None, 10, 20],\n",
       "                                        'rf__n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001F9C3F584F0>},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distributions = {\n",
    "    'rf__n_estimators': randint(100, 300),\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions,\n",
    "    n_iter=4,          # Only try 4 random combinations\n",
    "    cv=2,              # Reduce folds to 2 for faster time\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d7abcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'rf__max_depth': None, 'rf__n_estimators': 120}\n"
     ]
    }
   ],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "print(\"Best parameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e3b64",
   "metadata": {},
   "source": [
    "To optimize the Random Forest classifier, hyperparameter tuning was performed using RandomizedSearchCV. This approach tested different combinations of the number of trees (n_estimators) and maximum tree depth (max_depth) with cross-validation to identify the most effective configuration. The tuning process found that using 114 trees with no maximum depth constraint produced the best cross-validated results, providing a balance between model complexity and performance. This tuned configuration was then used to retrain the final model for improved accuracy and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba7453",
   "metadata": {},
   "source": [
    "## 4.6 Retrain the model using the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2064a2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "Best parameters: {'rf__max_depth': None, 'rf__n_estimators': 120}\n",
      "\n",
      "Tuned Random Forest — Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.88      0.83      0.86      2150\n",
      "    Negative       0.89      0.93      0.91      3876\n",
      "     Neutral       0.91      0.88      0.90      3037\n",
      "    Positive       0.88      0.89      0.89      3292\n",
      "\n",
      "    accuracy                           0.89     12355\n",
      "   macro avg       0.89      0.88      0.89     12355\n",
      "weighted avg       0.89      0.89      0.89     12355\n",
      "\n",
      "\n",
      "Tuned Random Forest — Confusion Matrix:\n",
      "\n",
      "[[1786  125   87  152]\n",
      " [  84 3595   73  124]\n",
      " [  74  151 2679  133]\n",
      " [  83  171   96 2942]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['Text'], \n",
    "    df['Sentiment'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define base pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=2000, ngram_range=(1, 2))),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_distributions = {\n",
    "    'rf__n_estimators': randint(100, 300),\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=4,        # Number of random combinations to try\n",
    "    cv=2,            # Folds for cross-validation\n",
    "    n_jobs=-1,       # Use all CPU cores\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to find best params\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "\n",
    "# Use best model to predict on test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_best_pred = best_model.predict(X_test)\n",
    "\n",
    "# Print performance\n",
    "print(\"\\nTuned Random Forest — Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_best_pred))\n",
    "print(\"\\nTuned Random Forest — Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_best_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "447d1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the hyperparameter tuned model\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('rf_pipeline_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa73ed",
   "metadata": {},
   "source": [
    "The tuned Random Forest classifier shows a significant improvement compared to the baseline and previous models, achieving an overall accuracy of 89% on the independent validation set. Precision, recall, and F1-scores for all sentiment classes remain consistently high, with particularly strong performance for the Neutral and Negative classes, which are often the most challenging in sentiment analysis. The balanced precision-recall across classes demonstrates that hyperparameter tuning has effectively enhanced the model’s generalization ability without overfitting. This makes the tuned Random Forest the best-performing model in this project so far, providing reliable, high-quality predictions that can confidently be used for real-time sentiment monitoring and business decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1f7a5",
   "metadata": {},
   "source": [
    " # 4.7 Predict on validation dataset using the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d90db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Random Forest on Validation Set — Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.91      0.89      0.90       171\n",
      "    Negative       0.93      0.94      0.93       266\n",
      "     Neutral       0.93      0.92      0.92       285\n",
      "    Positive       0.93      0.94      0.93       277\n",
      "\n",
      "    accuracy                           0.92       999\n",
      "   macro avg       0.92      0.92      0.92       999\n",
      "weighted avg       0.92      0.92      0.92       999\n",
      "\n",
      "\n",
      "Tuned Random Forest on Validation Set — Confusion Matrix:\n",
      "\n",
      "[[153   5   5   8]\n",
      " [  6 250   7   3]\n",
      " [  4  10 261  10]\n",
      " [  6   5   7 259]]\n"
     ]
    }
   ],
   "source": [
    "# Load tuned Random Forest pipeline\n",
    "with open(\"rf_pipeline_tuned.pkl\", \"rb\") as f:\n",
    "    rf_pipeline_tuned = pickle.load(f)\n",
    "\n",
    "# Load validation dataset\n",
    "val_file_path = 'data/twitter_validation.csv'\n",
    "val_df = pd.read_csv(val_file_path)\n",
    "\n",
    "# Predict using the tuned pipeline\n",
    "val_rf_tuned_preds = rf_pipeline_tuned.predict(val_df['Text'].astype(str))\n",
    "\n",
    "# Attach predictions\n",
    "val_df['Predicted_Sentiment_RF_Tuned'] = val_rf_tuned_preds\n",
    "\n",
    "# Evaluate predictions vs true labels\n",
    "print(\"Tuned Random Forest on Validation Set — Classification Report:\\n\")\n",
    "print(classification_report(val_df['Sentiment'], val_df['Predicted_Sentiment_RF_Tuned']))\n",
    "\n",
    "print(\"\\nTuned Random Forest on Validation Set — Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(val_df['Sentiment'], val_df['Predicted_Sentiment_RF_Tuned']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871c9e1",
   "metadata": {},
   "source": [
    "The tuned Random Forest model demonstrated excellent generalization when tested on the independent validation dataset, achieving an overall accuracy of 92% and consistently high precision, recall, and F1-scores across all sentiment classes. This strong performance on unseen data confirms that the model is robust and reliable, making it well-suited for real-world sentiment monitoring tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37188ca",
   "metadata": {},
   "source": [
    "**Visualize the distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa004081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_df['Predicted_Sentiment_RF_Tuned'].value_counts().plot(kind='bar', color='purple')\n",
    "plt.title(\"Predicted Sentiment Distribution (Random Forest)\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa96edc",
   "metadata": {},
   "source": [
    "The model predicts a fairly balanced distribution among Positive, Negative, and Neutral sentiments.\n",
    "\n",
    "Slightly fewer tweets classified as Irrelevant — which is common because Irrelevant tweets often share vocabulary with Neutral or Negative sentiments, making them harder to catch confidently.\n",
    "\n",
    "No major class skew, suggesting your model is not heavily biased toward a single sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8936d0",
   "metadata": {},
   "source": [
    "# **5.0 Conclusion and Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129bf35",
   "metadata": {},
   "source": [
    "## 5.1 Business Reccomendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df432b6",
   "metadata": {},
   "source": [
    "This section summarizes the overall findings and outcomes of the project and provides actionable recommendations based on the performance of the final sentiment analysis model. By reflecting on the results, we highlight how the insights gained can be used to support better decision-making, strengthen brand engagement strategies, and guide future improvements to the sentiment monitoring system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b11d9",
   "metadata": {},
   "source": [
    "**1. Develop Automated Alerts for Negative Sentiment Spikes**\n",
    "\n",
    "Configure the system to automatically send email or SMS alerts to key stakeholders whenever there is an unusual increase in negative sentiment. Early detection of a potential PR crisis allows the team to respond promptly, clarify misunderstandings, or escalate customer service interventions. This proactive approach can help protect the brand’s reputation and prevent small issues from becoming major problems.\n",
    "\n",
    "**2. Integrate Sentiment Insights into Marketing Campaigns**\n",
    "\n",
    "Use the analyzed sentiment data to inform the design and timing of marketing campaigns and social media content. Understanding how customers feel about different products or topics helps tailor messages to resonate better with the audience. This data-driven strategy can improve engagement rates and strengthen customer trust in the brand.\n",
    "\n",
    "**3. Share Insights with Product Development Teams**\n",
    "\n",
    "Deliver regular reports highlighting common complaints, feature requests, or positive feedback extracted from social media conversations. Product teams can use this real-world input to prioritize product improvements or new features that directly address customer needs. This ensures that future product iterations are more aligned with customer expectations.\n",
    "\n",
    "**4.  Implement Real-Time Monitoring Dashboards**\n",
    "\n",
    "Set up interactive dashboards that display up-to-date sentiment scores, trending topics, and keyword clouds related to the brand. This enables brand managers and communication teams to keep a pulse on customer opinions 24/7. With immediate access to insights, they can react quickly to both positive and negative trends as they develop.\n",
    "\n",
    "**5. Periodically Retrain the Model**\n",
    "\n",
    "Plan to retrain the sentiment classifier at regular intervals to keep it up-to-date with changing language, slang, and new trends in online conversation. This will maintain the model’s high accuracy and relevance over time. Retraining can be combined with expanding the dataset to include new topics as the brand evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9cf39",
   "metadata": {},
   "source": [
    "## 5.2 Limitations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62f74d",
   "metadata": {},
   "source": [
    "**1. Lack of Metadata**\n",
    "\n",
    "The dataset lacks information such as timestamps, user data, retweet/favorite counts, or geolocation. These could be valuable for contextualizing sentiment (e.g., during a PR crisis or product launch).\n",
    "\n",
    "**2. Incomplete Data (Missing Values)**\n",
    "\n",
    "The twitter_training.csv dataset has missing values in the tweet text column (Column 3), with around 686 entries missing (73,996 non-null out of 74,682 total). This can lead to biased training if not properly handled (e.g., removed or imputed).\n",
    "\n",
    "**3. Potential Duplicate or Near-Duplicate Entries**\n",
    "\n",
    "The dataset appears to include duplicate or near-duplicate tweets — entries that express the same idea with only slight variations in wording or punctuation. This redundancy can inflate performance metrics during model evaluation by making the dataset easier to predict. If not properly deduplicated or augmented, the presence of repetitive content may lead to overfitting and reduce the model’s generalization capabilities.\n",
    "\n",
    "**4. Limited Multilingual Coverage**\n",
    "\n",
    "The dataset is predominantly composed of English-language tweets, with only a small portion containing non-English characters. This language bias limits the model’s ability to generalize to global sentiment, especially for companies with international customer bases.the lack of multilingual representation can lead to a skewed or incomplete view of overall brand sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8a3dc",
   "metadata": {},
   "source": [
    "While this dataset is helpful for sentiment analysis for brands, it is far from flawless. Missing tweet text, missing metadata, subjective labeling, and sparse multilingual coverage are some concerns that indicate where the data could be lacking. Furthermore, partitioning of the tweets, possible duplicate records, and brand imbalanced representation can affect the validity and transferability of the findings. Even with these limitations, the dataset is a good foundation for investigating trend in sentiment and provides useful insight when handled carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119bf62",
   "metadata": {},
   "source": [
    "## 5.3 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3ad26",
   "metadata": {},
   "source": [
    "| Model                          | Accuracy | Precision (avg) | Recall (avg) | F1-Score (avg) | Notes                                      |\n",
    "|-------------------------------|----------|------------------|---------------|----------------|--------------------------------------------|\n",
    "| Logistic Regression (Baseline)| 68%      | 0.68             | 0.68          | 0.68           | Struggled with Irrelevant & Neutral classes|\n",
    "| Random Forest (Bi-grams)      | 89%      | 0.89             | 0.89          | 0.89           | Strong overall, improved Irrelevant recall |\n",
    "| XGBoost (Bi-grams)            | 64%      | 0.65             | 0.64          | 0.63           | Weaker generalization, esp. Irrelevant     |\n",
    "| Random Forest (Tuned)         | **89%**  | **0.89**         | **0.88**      | **0.88**       | Best model; balanced performance, stable   |\n",
    "| Random Forest (Tuned, Valid.) | **92%**  | **0.92**         | **0.92**      | **0.92**       | Final validation performance — optimal     |\n",
    "\n",
    "\n",
    "In this project, we successfully built a robust sentiment analysis system that can classify tweets about various brands into four sentiment categories. After testing multiple models, the Random Forest classifier — optimized through hyperparameter tuning — emerged as the most effective, achieving a 92% accuracy on the validation set. The model demonstrated strong and balanced performance across all sentiment classes, making it well-suited for real-world deployment. Despite limitations such as missing metadata and limited language coverage, this system lays a solid foundation for brand sentiment monitoring and provides actionable insights for stakeholders like Interbrand. With further iteration, it can be expanded into a powerful tool for proactive brand management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90d855",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
