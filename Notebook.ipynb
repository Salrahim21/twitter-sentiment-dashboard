{
 "cell_type": "markdown",
 "id": "fa7fd655",
 "metadata": {},
 "source": [
  "<img src=\"Data/sentiment_picture.png\" width=\"700\"/>\n",
  "\n",
  " # Social Media Sentiment Anlysis\n",
  " # Group 3 \n",
  "\n",
  "Authors\n",
  "  - Ibrahim Salim\n",
  "  - Abond Mwangi\n",
  "  - Abigail Muthenya\n",
  "  - Nelson Kamau\n"
 ]
},
{
 "cell_type": "markdown",
 "id": "f8854894",
 "metadata": {},
 "source": [
  "# **1.0 Introduction**"
 ]
},

  },
  {
   "cell_type": "markdown",
   "id": "83653aac",
   "metadata": {},
   "source": [
    "In today’s digital age, social media has become a critical channel for customers to share their opinions, experiences, and frustrations with brands and products. Platforms like Twitter generate vast amounts of unstructured text data daily, reflecting public sentiment in real time. For businesses, monitoring and understanding this sentiment is essential to protect brand reputation, respond to customer concerns, and inform strategic decisions. However, the sheer volume and velocity of social media data make it challenging to track and analyze sentiment manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab4906e",
   "metadata": {},
   "source": [
    "## 1.1 Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaef490",
   "metadata": {},
   "source": [
    "The goal of this project is to design and implement an automated system that will determine the sentiment in tweets which are directed towards specific brands such as Borderlands, Facebook, Amazon, and Microsoft. The system will be examining a large body of tweets, determining the overall sentiment in every tweet, and labeling  into sentiment categories such as Positive, Negative, Neutral, or Irrelevant. By leveraging Natural Language Processing and machine learning, the system will help organizations gain timely insights into how the public perceives their products and services.\n",
    "\n",
    "The results from this system can be employed to provide actionable insights to key stakeholders like brand managers, marketing departments, and community managers so that they can:\n",
    "\n",
    "- Monitor public opinion about their brand or product on an ongoing basis.\n",
    "\n",
    "- Recognize future potential issues or spikes in negative sentiment.\n",
    "\n",
    "- Measure the impact of marketing initiatives or product releases.\n",
    "\n",
    "- Compare sentiment trends relative to competition.\n",
    "\n",
    "The project will make use of a labeled set of tweets for training and validation. The system's performance will be measured by how well the system will be able to classify sentiment in order to instill confidence in businesses with regard to insights produced. Ultimately, the project demonstrates how data-driven sentiment analysis can facilitate proactive decision-making in brand management and customer engagement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb567871",
   "metadata": {},
   "source": [
    "## 1.2 The Business Stakeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36850f",
   "metadata": {},
   "source": [
    "This project has been initiated at the request of **Interbrand**, a global leading brand consultancy that helps businesses develop, manage, and protect their brand reputation. Interbrand brand managers monitor how the public perceives their clients' brands through all forms of media, including social media sites like Twitter. With the overwhelming amount of real-time customer discussions online, Interbrand's brand managers require an automated solution that can efficiently classify sentiment in tweets that talk about their clients' services or products. \n",
    "\n",
    "The other key stakeholders are **Interbrand**'s marketing analysts and PR teams, who will utilize these findings to make campaign strategy, crisis management strategy, and competitor benchmarking reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a511dd",
   "metadata": {},
   "source": [
    "## 1.3 Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16523bf7",
   "metadata": {},
   "source": [
    "In a time where public opinion can shift in a matter of seconds, brands need to be constantly on guard for what people are saying about them online. Social networks like Twitter generate massive streams of live comment, complaint, compliment, and conversation that can influence a brand's reputation, customer loyalty, and market position.\n",
    "\n",
    "But it is not practical and scalable to track and analyze manually thousands of tweets each day. This is very manual-intensive for brand managers and consultancies like **Interbrand** who are responsible for overseeing many brands in various industries and markets. Without a reliable way of monitoring the sentiment from these discussions, businesses are at risk of missing early warning signs of negative trends, missing important customer insights, or failing to properly gauge the full effects of their campaigns.\n",
    "\n",
    "This project answers that challenge by developing an automated sentiment analysis system that is able to classify tweets mentioning a brand into precise sentiment categories: **Positive, Negative, Neutral, or Irrelevant**. Through transforming raw, unstructured text data into actionable information, this system will enable brand managers to:\n",
    "\n",
    "- Track sentiment trends over time.\n",
    "\n",
    "- Identify sudden spikes in negative or positive remarks.\n",
    "\n",
    "- Respond quickly to breaking issues or crises.\n",
    "\n",
    "- Use data-driven insights to make decisions regarding communication strategies, product development, and customer engagement.\n",
    "\n",
    "Ultimately, the outcome is to allow brand managers to make timely, data-informed decisions that drive better decision-making and help them defend and build their brands in a changing digital world. By the end of this project we hope to uncover questions such as:\n",
    "\n",
    "- What is the overall sentiment towards the brand(s) on Twitter over a given period?\n",
    "\n",
    "- Are there specific topics, events, or product launches that cause spikes in positive or negative sentiment?\n",
    "\n",
    "- Can sudden surges in negative sentiment be detected early enough to enable timely response by customer support or PR teams?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a50190",
   "metadata": {},
   "source": [
    "## 1.4 Project Objectives "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf7339",
   "metadata": {},
   "source": [
    "The primary objective of this project is to develop an **automated sentiment analysis system** capable of classifying tweets that mention specific brands into distinct sentiment categories: **Positive, Negative, Neutral, or Irrelevant**. This will enable **brand managers and consultancies** like Interbrand to transform large volumes of unstructured social media data into actionable insights.\n",
    "\n",
    "To achieve this, the project will pursue the following specific objectives:\n",
    "\n",
    "- Build and train a reliable sentiment classification model using a labeled dataset of tweets that mention various brands, including Borderlands, Facebook, Amazon, and others.\n",
    "\n",
    "- Validate and evaluate the model’s performance to ensure it achieves high accuracy and can generalize well to new, unseen tweets.\n",
    "\n",
    "- Provide clear, interpretable outputs that brand managers can use to monitor brand health, inform marketing and PR strategies, and respond quickly to emerging issues.\n",
    "\n",
    "- Develop recommendations for how this system could be integrated into the stakeholders’ existing workflows for continuous, real-time sentiment tracking.\n",
    "\n",
    "By fulfilling these objectives, the project aims to demonstrate how automated sentiment analysis can support proactive, data-driven brand management in an increasingly fast-paced digital environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341de849",
   "metadata": {},
   "source": [
    "# **2.0 Data Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540a67f",
   "metadata": {},
   "source": [
    "The dataset used in this project was sourced from **Kaggle’s Twitter Entity Sentiment Analysis collection**. It includes labeled tweets that refer to different brands, firms, or products, which were categorized into one of four sentiment types: **Positive, Negative, Neutral, or Irrelevant**.\n",
    "\n",
    "The dataset includes two training and validation CSV files combined, which have over 75,000 tweets. The shape and composition of this dataset are important for determining how to build a robust sentiment classification model. This section will describe salient characteristics of the data, detect any potential issues (such as class imbalance or redundant entries), and lay the foundation for the data preprocessing and modeling sections that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fbda1",
   "metadata": {},
   "source": [
    "## 2.1 Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ccc85",
   "metadata": {},
   "source": [
    "The data includes two CSV files, one for **training (twitter_training.csv)** and one for **validation (twitter_validation.csv)** ,with a total of over 75,000 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d0713",
   "metadata": {},
   "source": [
    "**twitter_training.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf72c73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Positive</th>\n",
       "      <th>im getting on borderlands and i will murder you all ,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2401  Borderlands  Positive  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "...     ...          ...       ...   \n",
       "74676  9200       Nvidia  Positive   \n",
       "74677  9200       Nvidia  Positive   \n",
       "74678  9200       Nvidia  Positive   \n",
       "74679  9200       Nvidia  Positive   \n",
       "74680  9200       Nvidia  Positive   \n",
       "\n",
       "      im getting on borderlands and i will murder you all ,  \n",
       "0      I am coming to the borders and I will kill you...     \n",
       "1      im getting on borderlands and i will kill you ...     \n",
       "2      im coming on borderlands and i will murder you...     \n",
       "3      im getting on borderlands 2 and i will murder ...     \n",
       "4      im getting into borderlands and i can murder y...     \n",
       "...                                                  ...     \n",
       "74676  Just realized that the Windows partition of my...     \n",
       "74677  Just realized that my Mac window partition is ...     \n",
       "74678  Just realized the windows partition of my Mac ...     \n",
       "74679  Just realized between the windows partition of...     \n",
       "74680  Just like the windows partition of my Mac is l...     \n",
       "\n",
       "[74681 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training CSV file\n",
    "file_path = 'data/twitter_training.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe72ca",
   "metadata": {},
   "source": [
    "**twitter_validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140d116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3364</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>Irrelevant</th>\n",
       "      <th>I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>352</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4371</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4433</td>\n",
       "      <td>Google</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6273</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hi @EAHelp I’ve had Madeleine McCann in my cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>4891</td>\n",
       "      <td>GrandTheftAuto(GTA)</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4359</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2652</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>8069</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>6960</td>\n",
       "      <td>johnson&amp;johnson</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     3364             Facebook  Irrelevant  \\\n",
       "0     352               Amazon     Neutral   \n",
       "1    8312            Microsoft    Negative   \n",
       "2    4371                CS-GO    Negative   \n",
       "3    4433               Google     Neutral   \n",
       "4    6273                 FIFA    Negative   \n",
       "..    ...                  ...         ...   \n",
       "994  4891  GrandTheftAuto(GTA)  Irrelevant   \n",
       "995  4359                CS-GO  Irrelevant   \n",
       "996  2652          Borderlands    Positive   \n",
       "997  8069            Microsoft    Positive   \n",
       "998  6960      johnson&johnson     Neutral   \n",
       "\n",
       "    I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣  \n",
       "0    BBC News - Amazon boss Jeff Bezos rejects clai...                                                                                                                                                                                                  \n",
       "1    @Microsoft Why do I pay for WORD when it funct...                                                                                                                                                                                                  \n",
       "2    CSGO matchmaking is so full of closet hacking,...                                                                                                                                                                                                  \n",
       "3    Now the President is slapping Americans in the...                                                                                                                                                                                                  \n",
       "4    Hi @EAHelp I’ve had Madeleine McCann in my cel...                                                                                                                                                                                                  \n",
       "..                                                 ...                                                                                                                                                                                                  \n",
       "994  ⭐️ Toronto is the arts and culture capital of ...                                                                                                                                                                                                  \n",
       "995  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...                                                                                                                                                                                                  \n",
       "996  Today sucked so it’s time to drink wine n play...                                                                                                                                                                                                  \n",
       "997  Bought a fraction of Microsoft today. Small wins.                                                                                                                                                                                                  \n",
       "998  Johnson & Johnson to stop selling talc baby po...                                                                                                                                                                                                  \n",
       "\n",
       "[999 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the validation CSV file \n",
    "val_df = pd.read_csv('Data/twitter_validation.csv')\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240830f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2401', 'Borderlands', 'Positive',\n",
       "       'im getting on borderlands and i will murder you all ,'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the columns of the training DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcbe9dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['3364', 'Facebook', 'Irrelevant',\n",
       "       'I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the columns of the validation DataFrame\n",
    "val_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff534fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Entity', 'Sentiment', 'Text'], dtype='object')\n",
      "Index(['ID', 'Entity', 'Sentiment', 'Text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Change the column names to match the expected format\n",
    "\n",
    "df.columns = ['ID', 'Entity', 'Sentiment', 'Text']\n",
    "val_df.columns = ['ID', 'Entity', 'Sentiment', 'Text']\n",
    "\n",
    "# Display the updated columns\n",
    "print(df.columns)\n",
    "print(val_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f54e4a",
   "metadata": {},
   "source": [
    "## 2.2 Data Dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a78ffc",
   "metadata": {},
   "source": [
    "- **ID**: A unique identifier for the tweet.\n",
    "\n",
    "- **Entity**: The brand, company, or product mentioned (e.g., Borderlands, Facebook, Amazon).\n",
    "\n",
    "- **Sentiment**: The sentiment label assigned to the tweet.\n",
    "\n",
    "- **Tweet Text**: The actual tweet content, representing the raw, unstructured text data to be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a8fc6",
   "metadata": {},
   "source": [
    "## 2.3 Data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d58eec8",
   "metadata": {},
   "source": [
    "Here we shall be exploring the datasets to have better understanding about what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d2fa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74681 entries, 0 to 74680\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ID         74681 non-null  int64 \n",
      " 1   Entity     74681 non-null  object\n",
      " 2   Sentiment  74681 non-null  object\n",
      " 3   Text       73995 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0fdab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 999 entries, 0 to 998\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ID         999 non-null    int64 \n",
      " 1   Entity     999 non-null    object\n",
      " 2   Sentiment  999 non-null    object\n",
      " 3   Text       999 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 31.3+ KB\n"
     ]
    }
   ],
   "source": [
    "val_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4f3b9",
   "metadata": {},
   "source": [
    "**Check for missing entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09c4c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0.000000\n",
       "Entity       0.000000\n",
       "Sentiment    0.000000\n",
       "Text         0.918574\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the percentage of missing values per column for the training dataset\n",
    "100*(df.isnull().sum()/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b2e8cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0.0\n",
       "Entity       0.0\n",
       "Sentiment    0.0\n",
       "Text         0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the percentage of missing values per column for the validation dataset\n",
    "100*(val_df.isnull().sum()/len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe1b71",
   "metadata": {},
   "source": [
    "**Check for duplicated entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ce7ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for duplicated entries in the training dataset\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4acf9637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicated entries in the validation dataset\n",
    "val_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b0a45",
   "metadata": {},
   "source": [
    "# **3.0 Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8632e5",
   "metadata": {},
   "source": [
    "Before building a stable sentiment classification model, it is important to pre-process the raw Twitter data for analysis. This stage involves cleaning, transforming, and structuring the dataset to ensure it is suitable for machine learning. The primary tasks are handling missing or duplicate records, normalizing text data and class imbalance handling. Effective data preparation lays the groundwork for building a robust and accurate sentiment analysis system that aligns with the project’s business objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a209c",
   "metadata": {},
   "source": [
    "## 3.1 Handling missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2df33bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212d355",
   "metadata": {},
   "source": [
    " ## 3.2 Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "601f57d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kamau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kamau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kamau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # remove mentions\n",
    "        text = re.sub(r'#[A-Za-z0-9_]+', '', text)  # remove hashtags\n",
    "        text = re.sub(r'http\\S+', '', text)         # remove URLs\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)        # remove non-letters\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [t for t in tokens if t not in self.stop_words]\n",
    "        tokens = [self.lemmatizer.lemmatize(t, pos='v') for t in tokens]\n",
    "        return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0a1fa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  I am coming to the borders and I will kill you...   \n",
      "1  im getting on borderlands and i will kill you ...   \n",
      "2  im coming on borderlands and i will murder you...   \n",
      "3  im getting on borderlands 2 and i will murder ...   \n",
      "4  im getting into borderlands and i can murder y...   \n",
      "\n",
      "                   clean_text  \n",
      "0            come border kill  \n",
      "1     im get borderlands kill  \n",
      "2  im come borderlands murder  \n",
      "3   im get borderlands murder  \n",
      "4   im get borderlands murder  \n"
     ]
    }
   ],
   "source": [
    "cleaner = TextCleaner()\n",
    "\n",
    "df['clean_text'] = df['Text'].apply(cleaner.clean)\n",
    "print(df[['Text', 'clean_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4dd97",
   "metadata": {},
   "source": [
    "## 3.3 Handling the duplicated entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33206dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12221"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count duplicate cleaned tweets\n",
    "df.duplicated(subset=['clean_text']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6926ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on cleaned text\n",
    "df.drop_duplicates(subset=['clean_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f04820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(\"twitter_training_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6139147",
   "metadata": {},
   "source": [
    "## 3.4 Check for class imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10bd7029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "Negative      30.580827\n",
      "Positive      26.519248\n",
      "Neutral       25.182115\n",
      "Irrelevant    17.717810\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['Sentiment'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8826e",
   "metadata": {},
   "source": [
    "# **4.0 Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75176a",
   "metadata": {},
   "source": [
    "With the data cleaned, prepared, and explored, the next step is to develop and evaluate machine learning models that can accurately classify the sentiment of tweets. This stage involves transforming the text data into numerical features using **natural language processing (NLP)** techniques, selecting suitable algorithms, training the models on labeled examples, and validating their performance.\n",
    "\n",
    "The goal is to identify a model that can generalize well to new, unseen tweets and reliably predict whether a tweet’s sentiment is Positive, Negative, Neutral, or Irrelevant. This section will cover the text vectorization process, model selection, training, and performance evaluation to ensure the final sentiment classifier meets the business requirements for accuracy and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe30802",
   "metadata": {},
   "source": [
    "## 4.1 Vectorize text (TF-IDF) and prepare the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d0cee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "#  Prepare target variable\n",
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0504a795",
   "metadata": {},
   "source": [
    " ## 4.2 Building the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd89d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.65      0.53      0.58      2150\n",
      "    Negative       0.72      0.78      0.75      3876\n",
      "     Neutral       0.65      0.62      0.64      3037\n",
      "    Positive       0.68      0.72      0.70      3292\n",
      "\n",
      "    accuracy                           0.68     12355\n",
      "   macro avg       0.67      0.66      0.67     12355\n",
      "weighted avg       0.68      0.68      0.68     12355\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1140  373  278  359]\n",
      " [ 186 3022  361  307]\n",
      " [ 229  454 1897  457]\n",
      " [ 197  337  391 2367]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5e616",
   "metadata": {},
   "source": [
    "**Prediction example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5df1aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Assuming the TextCleaner class is already defined and imported\n",
    "\n",
    "# Create cleaner object\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "# Example text\n",
    "example = [\"I absolutely love the new Borderlands game!\"]\n",
    "\n",
    "# Clean the text\n",
    "example_clean = [cleaner.clean(example[0])]\n",
    "\n",
    "# Vectorize and predict\n",
    "example_vec = vectorizer.transform(example_clean)\n",
    "prediction = model.predict(example_vec)\n",
    "\n",
    "print(\"\\nPredicted sentiment:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d31ff33",
   "metadata": {},
   "source": [
    "The baseline model demonstrates a reasonable starting point, achieving an overall accuracy of approximately 68% and performing strongest on Negative and Positive sentiment classes. However, the relatively lower recall for the Irrelevant and Neutral categories indicates that the model struggles to distinguish these sentiments reliably. This suggests that further improvements in text preprocessing, feature extraction, and model tuning could help enhance overall performance and reduce misclassifications. In the next steps, additional techniques such as advanced vectorization, hyperparameter optimization, or handling potential class imbalances will be explored to build a more robust and generalizable sentiment classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af80f95",
   "metadata": {},
   "source": [
    "#  Random Forest (with bi-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aaed2703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.88      0.83      0.85      2150\n",
      "    Negative       0.89      0.93      0.91      3876\n",
      "     Neutral       0.92      0.88      0.90      3037\n",
      "    Positive       0.87      0.89      0.88      3292\n",
      "\n",
      "    accuracy                           0.89     12355\n",
      "   macro avg       0.89      0.88      0.89     12355\n",
      "weighted avg       0.89      0.89      0.89     12355\n",
      "\n",
      "[[1778  126   87  159]\n",
      " [  81 3589   74  132]\n",
      " [  71  147 2677  142]\n",
      " [  86  174   87 2945]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=2000, ngram_range=(1, 2))),\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3aacccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the pipeline\n",
    "\n",
    "with open(\"rf_pipeline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf_pipeline, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ff5e2",
   "metadata": {},
   "source": [
    "#  XGBoost (with bi-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd298396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:44:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:01:29] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { use_label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.70      0.36      0.47      2150\n",
      "    Negative       0.60      0.83      0.70      3876\n",
      "     Neutral       0.68      0.55      0.61      3037\n",
      "    Positive       0.65      0.68      0.66      3292\n",
      "\n",
      "    accuracy                           0.64     12355\n",
      "   macro avg       0.66      0.60      0.61     12355\n",
      "weighted avg       0.65      0.64      0.63     12355\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      " [[ 772  707  222  449]\n",
      " [  89 3211  287  289]\n",
      " [ 113  761 1682  481]\n",
      " [ 129  646  290 2227]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Encode labels first — handled outside the pipeline since LabelEncoder isn't a pipeline step\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline: Vectorizer + Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n",
    "])\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_encoded = pipeline.predict(X_test)\n",
    "\n",
    "# Decode labels for reporting\n",
    "y_pred_decoded = le.inverse_transform(y_pred_encoded)\n",
    "y_test_decoded = le.inverse_transform(y_test)\n",
    "\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test_decoded, y_pred_decoded))\n",
    "print(\"XGBoost Confusion Matrix:\\n\", confusion_matrix(y_test_decoded, y_pred_decoded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14954805",
   "metadata": {},
   "source": [
    "| Metric        | Random Forest         | XGBoost    |\n",
    "| ------------- | --------------------- | ---------- |\n",
    "| Accuracy      | **89%**               | 64%        |\n",
    "| Macro F1      | **0.89**              | 0.61       |\n",
    "| Best Class    | Negative / Irrelevant | Negative   |\n",
    "| Weakest Class | Irrelevant (recall)   | Irrelevant |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c2a784",
   "metadata": {},
   "source": [
    "Your Random Forest model is clearly stronger right now on this feature set (TF-IDF with bigrams).\n",
    "\n",
    "XGBoost struggled here probably because it is more sensitive to class imbalance and requires more hyperparameter tuning to match RF in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e2258",
   "metadata": {},
   "source": [
    " # Load & predict on validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59b70215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Predicted_Sentiment_RF\n",
      "0  BBC News - Amazon boss Jeff Bezos rejects clai...                Neutral\n",
      "1  @Microsoft Why do I pay for WORD when it funct...               Negative\n",
      "2  CSGO matchmaking is so full of closet hacking,...               Negative\n",
      "3  Now the President is slapping Americans in the...                Neutral\n",
      "4  Hi @EAHelp I’ve had Madeleine McCann in my cel...               Negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# 1️ Load Random Forest pipeline (already trained and saved)\n",
    "with open(\"rf_pipeline.pkl\", \"rb\") as f:\n",
    "    rf_pipeline = pickle.load(f)\n",
    "\n",
    "# 2️ Load validation dataset\n",
    "val_file_path = 'data/twitter_validation.csv'\n",
    "val_df = pd.read_csv(val_file_path)\n",
    "\n",
    "# 3️ Rename columns if needed (only if you know exactly what they are)\n",
    "val_df.columns = ['ID', 'Entity', 'Sentiment', 'Text']\n",
    "\n",
    "# 4️ Predict using the pipeline (pipeline already includes vectorizer + classifier)\n",
    "val_rf_preds = rf_pipeline.predict(val_df['Text'].astype(str))\n",
    "\n",
    "# 5️ Attach predictions to the DataFrame\n",
    "val_df['Predicted_Sentiment_RF'] = val_rf_preds\n",
    "\n",
    "# 6️ Preview\n",
    "print(val_df[['Text', 'Predicted_Sentiment_RF']].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de353400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted_Sentiment_RF\n",
      "Neutral       287\n",
      "Positive      282\n",
      "Negative      264\n",
      "Irrelevant    166\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(val_df['Predicted_Sentiment_RF'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18339eae",
   "metadata": {},
   "source": [
    "Once we trained our Random Forest model, it was already performing quite well — achieving approximately 89% accuracy on the validation set. This strong baseline gave us confidence that the model was learning meaningful patterns from the data. However, we also recognized that with some careful hyperparameter tuning, we could potentially improve performance even further to support more accurate and reliable predictions.\n",
    "\n",
    "Initially, we attempted to use GridSearchCV, which exhaustively tests all combinations of hyperparameters. While thorough, this approach proved to be too time-consuming given the size of our dataset and the number of parameter options. To balance efficiency with optimization, we instead used RandomizedSearchCV — a faster, more practical method that samples a limited number of random combinations from the parameter space. This allowed us to fine-tune the model more efficiently while still seeking improved predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c74862a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:  1.2min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  3.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=Pipeline(steps=[('tfidf',\n",
       "                                              TfidfVectorizer(max_features=2000,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           2))),\n",
       "                                             ('rf',\n",
       "                                              RandomForestClassifier(class_weight='balanced',\n",
       "                                                                     n_estimators=200,\n",
       "                                                                     random_state=42))]),\n",
       "                   n_iter=4, n_jobs=-1,\n",
       "                   param_distributions={'rf__max_depth': [None, 10, 20],\n",
       "                                        'rf__n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000020A819583A0>},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distributions = {\n",
    "    'rf__n_estimators': randint(100, 300),\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions,\n",
    "    n_iter=4,          # Only try 4 random combinations\n",
    "    cv=2,              # Reduce folds to 2 for faster time\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "069d0a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'rf__max_depth': None, 'rf__n_estimators': 114}\n"
     ]
    }
   ],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "print(\"Best parameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba0b42",
   "metadata": {},
   "source": [
    " # Visualize the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa004081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_df['Predicted_Sentiment_RF'].value_counts().plot(kind='bar', color='purple')\n",
    "plt.title(\"Predicted Sentiment Distribution (Random Forest)\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa96edc",
   "metadata": {},
   "source": [
    "The model predicts a fairly balanced distribution among Positive, Negative, and Neutral sentiments.\n",
    "\n",
    "Slightly fewer tweets classified as Irrelevant — which is common because Irrelevant tweets often share vocabulary with Neutral or Negative sentiments, making them harder to catch confidently.\n",
    "\n",
    "No major class skew, suggesting your model is not heavily biased toward a single sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5193f6",
   "metadata": {},
   "source": [
    "# **5. Recommendations**\n",
    "1. Monitor and refine over time: As the model is exposed to new tweet streams, set up periodic evaluation cycles to ensure continued performance and relevance, especially as language and sentiment trends evolve.\n",
    "\n",
    "2. Expand to multilingual support: If targeting broader markets, future versions can incorporate multilingual sentiment detection to cover non-English tweets.\n",
    "\n",
    "3. Integrate with dashboards or alert systems: Connect the model to a real-time dashboard or alert system to flag spikes in negative sentiment or trending topics for rapid response.\n",
    "\n",
    "4. Apply sentiment insights across departments: Use aggregated sentiment trends to inform marketing strategies, customer support priorities, and product development feedback loops.\n",
    "\n",
    "5. Leverage advanced models in the background: Over time, the system can experiment with more advanced architectures (e.g., transformers) for deeper contextual understanding while maintaining the current model in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53046b",
   "metadata": {},
   "source": [
    "# **6. Conclusion**\n",
    "Once deployed, the model will enable the client to automatically classify large volumes of tweets into actionable sentiment categories **—Positive, Negative, Neutral, and Irrelevant—** providing a scalable and efficient solution for monitoring public perception in real time. It supports critical use cases such as tracking brand sentiment during product launches, identifying sudden spikes in negative feedback, and filtering out noise to focus on high-value conversations.\n",
    "\n",
    "By delivering consistent, multi-class sentiment insights, the system serves as a reliable foundation for automated sentiment analysis, empowering teams across marketing, customer service, and strategy to make informed decisions. As the business evolves, the model can be further refined and integrated into broader analytics pipelines to extract deeper insights, support campaign evaluation, and strengthen long-term brand management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74008b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
